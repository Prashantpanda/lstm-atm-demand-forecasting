{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>atm_name</th>\n",
       "      <th>weekday</th>\n",
       "      <th>festival_religion</th>\n",
       "      <th>working_day</th>\n",
       "      <th>holiday_sequence</th>\n",
       "      <th>trans_date_set</th>\n",
       "      <th>trans_month</th>\n",
       "      <th>trans_year</th>\n",
       "      <th>prevweek_mean</th>\n",
       "      <th>total_amount_withdrawn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>648600.0</td>\n",
       "      <td>897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>648600.0</td>\n",
       "      <td>826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>648600.0</td>\n",
       "      <td>754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>648600.0</td>\n",
       "      <td>834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>648600.0</td>\n",
       "      <td>575300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        atm_name    weekday festival_religion working_day  \\\n",
       "0          11  Mount Road ATM     MONDAY                NH           W   \n",
       "1          16  Mount Road ATM    TUESDAY                NH           W   \n",
       "2          21  Mount Road ATM  WEDNESDAY                NH           W   \n",
       "3          26  Mount Road ATM   THURSDAY                NH           W   \n",
       "4          31  Mount Road ATM     FRIDAY                NH           W   \n",
       "\n",
       "  holiday_sequence  trans_date_set  trans_month  trans_year  prevweek_mean  \\\n",
       "0              WWW               1            1        2011       648600.0   \n",
       "1              WWW               1            1        2011       648600.0   \n",
       "2              WWW               1            1        2011       648600.0   \n",
       "3              WWW               2            1        2011       648600.0   \n",
       "4              WWW               2            1        2011       648600.0   \n",
       "\n",
       "   total_amount_withdrawn  \n",
       "0                  897100  \n",
       "1                  826000  \n",
       "2                  754400  \n",
       "3                  834200  \n",
       "4                  575300  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:10].values\n",
    "y = dataset.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_0 = LabelEncoder()\n",
    "X[:,0] = labelencoder_X_0.fit_transform(X[:, 0])\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:,1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:,2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "labelencoder_X_3 = LabelEncoder()\n",
    "X[:,3] = labelencoder_X_3.fit_transform(X[:, 3])\n",
    "labelencoder_X_4 = LabelEncoder()\n",
    "X[:,4] = labelencoder_X_4.fit_transform(X[:, 4])\n",
    "labelencoder_X_5 = LabelEncoder()\n",
    "X[:,5] = labelencoder_X_5.fit_transform(X[:, 5])\n",
    "# Encode variable with one hotencoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1,2,3,4,5])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=33, units=25, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=20, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=15, kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=8, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1, kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1, kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kavish\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "1795/1795 [==============================] - 1s 445us/step - loss: 336817389659.2758\n",
      "Epoch 2/100\n",
      "1795/1795 [==============================] - 0s 217us/step - loss: 332842399190.6407\n",
      "Epoch 3/100\n",
      "1795/1795 [==============================] - 0s 218us/step - loss: 252763638518.7298\n",
      "Epoch 4/100\n",
      "1795/1795 [==============================] - 0s 221us/step - loss: 76036292436.8579\n",
      "Epoch 5/100\n",
      "1795/1795 [==============================] - 0s 223us/step - loss: 45605522372.1003\n",
      "Epoch 6/100\n",
      "1795/1795 [==============================] - 0s 272us/step - loss: 42083956621.9053\n",
      "Epoch 7/100\n",
      "1795/1795 [==============================] - 0s 205us/step - loss: 40326958445.1031\n",
      "Epoch 8/100\n",
      "1795/1795 [==============================] - 1s 307us/step - loss: 39049042256.5794\n",
      "Epoch 9/100\n",
      "1795/1795 [==============================] - 0s 271us/step - loss: 38147850686.3955\n",
      "Epoch 10/100\n",
      "1795/1795 [==============================] - 1s 279us/step - loss: 37437915155.9666\n",
      "Epoch 11/100\n",
      "1795/1795 [==============================] - 1s 280us/step - loss: 36928921970.8078\n",
      "Epoch 12/100\n",
      "1795/1795 [==============================] - 0s 274us/step - loss: 36507393848.3343\n",
      "Epoch 13/100\n",
      "1795/1795 [==============================] - 0s 267us/step - loss: 36119037258.8746\n",
      "Epoch 14/100\n",
      "1795/1795 [==============================] - 1s 398us/step - loss: 35939435462.9526\n",
      "Epoch 15/100\n",
      "1795/1795 [==============================] - 0s 251us/step - loss: 35684500498.5404\n",
      "Epoch 16/100\n",
      "1795/1795 [==============================] - 1s 313us/step - loss: 35492409184.2674\n",
      "Epoch 17/100\n",
      "1795/1795 [==============================] - 0s 257us/step - loss: 35285120581.8830\n",
      "Epoch 18/100\n",
      "1795/1795 [==============================] - 1s 344us/step - loss: 35196027350.6407\n",
      "Epoch 19/100\n",
      "1795/1795 [==============================] - 1s 279us/step - loss: 35044879168.8914\n",
      "Epoch 20/100\n",
      "1795/1795 [==============================] - 1s 383us/step - loss: 34947428440.4234\n",
      "Epoch 21/100\n",
      "1795/1795 [==============================] - 0s 263us/step - loss: 34810982959.0641\n",
      "Epoch 22/100\n",
      "1795/1795 [==============================] - 0s 258us/step - loss: 34699497240.9582\n",
      "Epoch 23/100\n",
      "1795/1795 [==============================] - 0s 243us/step - loss: 34612151390.1281\n",
      "Epoch 24/100\n",
      "1795/1795 [==============================] - 0s 267us/step - loss: 34569809716.0557\n",
      "Epoch 25/100\n",
      "1795/1795 [==============================] - 1s 295us/step - loss: 34472408779.9443\n",
      "Epoch 26/100\n",
      "1795/1795 [==============================] - 1s 364us/step - loss: 34337386541.6379\n",
      "Epoch 27/100\n",
      "1795/1795 [==============================] - 1s 325us/step - loss: 34397192517.1699\n",
      "Epoch 28/100\n",
      "1795/1795 [==============================] - 1s 294us/step - loss: 34313689213.5042\n",
      "Epoch 29/100\n",
      "1795/1795 [==============================] - 1s 305us/step - loss: 34265029167.0641\n",
      "Epoch 30/100\n",
      "1795/1795 [==============================] - 1s 318us/step - loss: 34149043433.8941\n",
      "Epoch 31/100\n",
      "1795/1795 [==============================] - 1s 359us/step - loss: 34126474998.7298\n",
      "Epoch 32/100\n",
      "1795/1795 [==============================] - 1s 295us/step - loss: 34096139917.1922\n",
      "Epoch 33/100\n",
      "1795/1795 [==============================] - 1s 365us/step - loss: 34057416347.4540\n",
      "Epoch 34/100\n",
      "1795/1795 [==============================] - 0s 277us/step - loss: 33997797304.6908\n",
      "Epoch 35/100\n",
      "1795/1795 [==============================] - 1s 292us/step - loss: 33911079847.5766\n",
      "Epoch 36/100\n",
      "1795/1795 [==============================] - 1s 310us/step - loss: 33919357598.3064\n",
      "Epoch 37/100\n",
      "1795/1795 [==============================] - 1s 370us/step - loss: 33827094613.5710\n",
      "Epoch 38/100\n",
      "1795/1795 [==============================] - 1s 360us/step - loss: 33836328412.3454\n",
      "Epoch 39/100\n",
      "1795/1795 [==============================] - 1s 366us/step - loss: 33861184434.9861\n",
      "Epoch 40/100\n",
      "1795/1795 [==============================] - 1s 376us/step - loss: 33735231652.0111\n",
      "Epoch 41/100\n",
      "1795/1795 [==============================] - 1s 406us/step - loss: 33699532651.6769\n",
      "Epoch 42/100\n",
      "1795/1795 [==============================] - 1s 408us/step - loss: 33744132212.9471\n",
      "Epoch 43/100\n",
      "1795/1795 [==============================] - 1s 499us/step - loss: 33671110598.9526\n",
      "Epoch 44/100\n",
      "1795/1795 [==============================] - 1s 305us/step - loss: 33609496975.3315\n",
      "Epoch 45/100\n",
      "1795/1795 [==============================] - 1s 353us/step - loss: 33643769604.9916\n",
      "Epoch 46/100\n",
      "1795/1795 [==============================] - 1s 311us/step - loss: 33527591530.9638\n",
      "Epoch 47/100\n",
      "1795/1795 [==============================] - 1s 297us/step - loss: 33453796082.4513\n",
      "Epoch 48/100\n",
      "1795/1795 [==============================] - 1s 344us/step - loss: 33570000194.3175\n",
      "Epoch 49/100\n",
      "1795/1795 [==============================] - 1s 372us/step - loss: 33498411373.1031\n",
      "Epoch 50/100\n",
      "1795/1795 [==============================] - 1s 357us/step - loss: 33461303053.5487 0s - loss: 33504106837.\n",
      "Epoch 51/100\n",
      "1795/1795 [==============================] - 1s 361us/step - loss: 33446260650.4290\n",
      "Epoch 52/100\n",
      "1795/1795 [==============================] - 1s 313us/step - loss: 33407914822.5961\n",
      "Epoch 53/100\n",
      "1795/1795 [==============================] - 1s 295us/step - loss: 33340901287.5766\n",
      "Epoch 54/100\n",
      "1795/1795 [==============================] - 1s 323us/step - loss: 33347446439.5766\n",
      "Epoch 55/100\n",
      "1795/1795 [==============================] - 1s 333us/step - loss: 33311298474.4290\n",
      "Epoch 56/100\n",
      "1795/1795 [==============================] - 1s 292us/step - loss: 33278156406.3733\n",
      "Epoch 57/100\n",
      "1795/1795 [==============================] - 1s 288us/step - loss: 33356019298.4067\n",
      "Epoch 58/100\n",
      "1795/1795 [==============================] - 1s 287us/step - loss: 33254387004.6128\n",
      "Epoch 59/100\n",
      "1795/1795 [==============================] - 1s 369us/step - loss: 33254814252.2117\n",
      "Epoch 60/100\n",
      "1795/1795 [==============================] - 1s 328us/step - loss: 33239354365.1476\n",
      "Epoch 61/100\n",
      "1795/1795 [==============================] - 1s 286us/step - loss: 33207373538.7632\n",
      "Epoch 62/100\n",
      "1795/1795 [==============================] - 0s 273us/step - loss: 33156661259.4095\n",
      "Epoch 63/100\n",
      "1795/1795 [==============================] - 1s 279us/step - loss: 33132098608.4903\n",
      "Epoch 64/100\n",
      "1795/1795 [==============================] - 1s 308us/step - loss: 33124386927.2423\n",
      "Epoch 65/100\n",
      "1795/1795 [==============================] - 1s 521us/step - loss: 33085617967.7772\n",
      "Epoch 66/100\n",
      "1795/1795 [==============================] - 1s 390us/step - loss: 33093649020.0780\n",
      "Epoch 67/100\n",
      "1795/1795 [==============================] - 1s 452us/step - loss: 33113175924.2340\n",
      "Epoch 68/100\n",
      "1795/1795 [==============================] - 1s 391us/step - loss: 33055411633.5599\n",
      "Epoch 69/100\n",
      "1795/1795 [==============================] - 1s 391us/step - loss: 32997623662.5292\n",
      "Epoch 70/100\n",
      "1795/1795 [==============================] - 1s 388us/step - loss: 32982838923.7660 0s - loss: 31530129\n",
      "Epoch 71/100\n",
      "1795/1795 [==============================] - 1s 348us/step - loss: 32916593410.1393\n",
      "Epoch 72/100\n",
      "1795/1795 [==============================] - 1s 312us/step - loss: 32892775697.8273\n",
      "Epoch 73/100\n",
      "1795/1795 [==============================] - 1s 309us/step - loss: 32841688429.1031\n",
      "Epoch 74/100\n",
      "1795/1795 [==============================] - 1s 298us/step - loss: 32783820819.9666\n",
      "Epoch 75/100\n",
      "1795/1795 [==============================] - 1s 300us/step - loss: 32859960804.9025\n",
      "Epoch 76/100\n",
      "1795/1795 [==============================] - 1s 305us/step - loss: 32854055379.7883\n",
      "Epoch 77/100\n",
      "1795/1795 [==============================] - 1s 290us/step - loss: 32868343394.4067\n",
      "Epoch 78/100\n",
      "1795/1795 [==============================] - 0s 270us/step - loss: 32795273829.2591\n",
      "Epoch 79/100\n",
      "1795/1795 [==============================] - 0s 268us/step - loss: 32835283948.0334\n",
      "Epoch 80/100\n",
      "1795/1795 [==============================] - 1s 280us/step - loss: 32810650652.5237\n",
      "Epoch 81/100\n",
      "1795/1795 [==============================] - 0s 272us/step - loss: 32796443861.9276\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795/1795 [==============================] - 0s 270us/step - loss: 32670923659.0529\n",
      "Epoch 83/100\n",
      "1795/1795 [==============================] - 0s 248us/step - loss: 32723843185.3816\n",
      "Epoch 84/100\n",
      "1795/1795 [==============================] - 0s 258us/step - loss: 32711979604.1448\n",
      "Epoch 85/100\n",
      "1795/1795 [==============================] - 0s 253us/step - loss: 32579283828.2340\n",
      "Epoch 86/100\n",
      "1795/1795 [==============================] - 0s 269us/step - loss: 32746009109.3928\n",
      "Epoch 87/100\n",
      "1795/1795 [==============================] - 0s 249us/step - loss: 32638971647.2869\n",
      "Epoch 88/100\n",
      "1795/1795 [==============================] - 0s 251us/step - loss: 32643921338.1170\n",
      "Epoch 89/100\n",
      "1795/1795 [==============================] - 0s 264us/step - loss: 32591069203.9666\n",
      "Epoch 90/100\n",
      "1795/1795 [==============================] - 0s 249us/step - loss: 32623740816.7577\n",
      "Epoch 91/100\n",
      "1795/1795 [==============================] - 0s 255us/step - loss: 32603855173.1699\n",
      "Epoch 92/100\n",
      "1795/1795 [==============================] - 0s 255us/step - loss: 32566746343.0418\n",
      "Epoch 93/100\n",
      "1795/1795 [==============================] - 1s 296us/step - loss: 32512093697.4262\n",
      "Epoch 94/100\n",
      "1795/1795 [==============================] - 1s 318us/step - loss: 32524969148.2563\n",
      "Epoch 95/100\n",
      "1795/1795 [==============================] - 1s 318us/step - loss: 32527407955.4318\n",
      "Epoch 96/100\n",
      "1795/1795 [==============================] - 1s 341us/step - loss: 32490173086.3064\n",
      "Epoch 97/100\n",
      "1795/1795 [==============================] - 1s 362us/step - loss: 32489090595.6546\n",
      "Epoch 98/100\n",
      "1795/1795 [==============================] - 1s 317us/step - loss: 32521064333.9053\n",
      "Epoch 99/100\n",
      "1795/1795 [==============================] - 1s 291us/step - loss: 32420221877.8384\n",
      "Epoch 100/100\n",
      "1795/1795 [==============================] - 0s 274us/step - loss: 32453348297.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e4e41d6550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'uniform', activation = 'relu', input_dim = 33))\n",
    "\n",
    "# Adding the 2 hidden layer\n",
    "classifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the 3 hidden layer\n",
    "classifier.add(Dense(output_dim = 15, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the 4 hidden layer\n",
    "classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'linear'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'linear'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y=530100, Predicted=[477318.44]\n",
      "Y=930900, Predicted=[773772.2]\n",
      "Y=781900, Predicted=[584802.8]\n",
      "Y=350900, Predicted=[434192.75]\n",
      "Y=462500, Predicted=[408456.34]\n",
      "Y=60600, Predicted=[368090.78]\n",
      "Y=703300, Predicted=[741309.94]\n",
      "Y=311900, Predicted=[565070.3]\n",
      "Y=546100, Predicted=[547802.2]\n",
      "Y=305000, Predicted=[300796.56]\n",
      "Y=506100, Predicted=[886360.6]\n",
      "Y=181300, Predicted=[190956.25]\n",
      "Y=642300, Predicted=[545189.94]\n",
      "Y=289000, Predicted=[214936.72]\n",
      "Y=699500, Predicted=[701346.3]\n",
      "Y=697100, Predicted=[560080.7]\n",
      "Y=478700, Predicted=[602404.8]\n",
      "Y=597600, Predicted=[418035.72]\n",
      "Y=679100, Predicted=[474660.3]\n",
      "Y=400800, Predicted=[492143.1]\n",
      "Y=251500, Predicted=[306787.7]\n",
      "Y=214700, Predicted=[172163.88]\n",
      "Y=148000, Predicted=[298347.78]\n",
      "Y=659200, Predicted=[533678.06]\n",
      "Y=606100, Predicted=[715579.]\n",
      "Y=199900, Predicted=[255740.94]\n",
      "Y=384200, Predicted=[280853.22]\n",
      "Y=244700, Predicted=[367395.8]\n",
      "Y=566200, Predicted=[580923.]\n",
      "Y=315900, Predicted=[400580.]\n",
      "Y=88500, Predicted=[593196.3]\n",
      "Y=115000, Predicted=[330693.38]\n",
      "Y=506300, Predicted=[651215.2]\n",
      "Y=290100, Predicted=[507796.56]\n",
      "Y=828000, Predicted=[625933.7]\n",
      "Y=473600, Predicted=[447634.12]\n",
      "Y=473700, Predicted=[498220.97]\n",
      "Y=310500, Predicted=[261968.56]\n",
      "Y=433800, Predicted=[547620.6]\n",
      "Y=718900, Predicted=[361108.25]\n",
      "Y=1020500, Predicted=[632103.6]\n",
      "Y=448700, Predicted=[357807.8]\n",
      "Y=127300, Predicted=[184973.45]\n",
      "Y=393900, Predicted=[522093.25]\n",
      "Y=183800, Predicted=[160287.45]\n",
      "Y=651200, Predicted=[602955.75]\n",
      "Y=358000, Predicted=[421042.53]\n",
      "Y=416300, Predicted=[401128.34]\n",
      "Y=161500, Predicted=[328682.06]\n",
      "Y=602300, Predicted=[584016.75]\n",
      "Y=271500, Predicted=[531873.5]\n",
      "Y=444300, Predicted=[582043.]\n",
      "Y=312900, Predicted=[484965.16]\n",
      "Y=668100, Predicted=[604400.94]\n",
      "Y=518900, Predicted=[490146.8]\n",
      "Y=592600, Predicted=[515606.12]\n",
      "Y=356300, Predicted=[409396.66]\n",
      "Y=419400, Predicted=[490882.78]\n",
      "Y=675200, Predicted=[527062.4]\n",
      "Y=724600, Predicted=[530816.1]\n",
      "Y=755900, Predicted=[758894.56]\n",
      "Y=668000, Predicted=[725196.5]\n",
      "Y=585300, Predicted=[712758.9]\n",
      "Y=709400, Predicted=[738243.44]\n",
      "Y=444300, Predicted=[587085.1]\n",
      "Y=515500, Predicted=[664041.44]\n",
      "Y=339300, Predicted=[446560.47]\n",
      "Y=365800, Predicted=[385169.94]\n",
      "Y=328000, Predicted=[141852.14]\n",
      "Y=674400, Predicted=[622245.1]\n",
      "Y=67600, Predicted=[195781.45]\n",
      "Y=300700, Predicted=[500724.6]\n",
      "Y=740300, Predicted=[614619.9]\n",
      "Y=619500, Predicted=[734570.3]\n",
      "Y=517500, Predicted=[481356.3]\n",
      "Y=83400, Predicted=[159176.97]\n",
      "Y=100, Predicted=[254770.19]\n",
      "Y=153400, Predicted=[661632.1]\n",
      "Y=335900, Predicted=[400156.97]\n",
      "Y=304600, Predicted=[417786.7]\n",
      "Y=441800, Predicted=[813855.06]\n",
      "Y=158100, Predicted=[261352.55]\n",
      "Y=447400, Predicted=[292106.78]\n",
      "Y=606000, Predicted=[597652.2]\n",
      "Y=918200, Predicted=[676837.]\n",
      "Y=672500, Predicted=[495261.44]\n",
      "Y=664300, Predicted=[591850.5]\n",
      "Y=305100, Predicted=[400166.94]\n",
      "Y=572700, Predicted=[491091.16]\n",
      "Y=555600, Predicted=[612439.9]\n",
      "Y=775300, Predicted=[597752.]\n",
      "Y=819100, Predicted=[867700.5]\n",
      "Y=604600, Predicted=[551370.7]\n",
      "Y=868800, Predicted=[695754.06]\n",
      "Y=339600, Predicted=[467570.38]\n",
      "Y=437000, Predicted=[624020.75]\n",
      "Y=439600, Predicted=[488310.88]\n",
      "Y=699700, Predicted=[507250.94]\n",
      "Y=50800, Predicted=[132260.7]\n",
      "Y=395200, Predicted=[446237.5]\n",
      "Y=338100, Predicted=[516986.72]\n",
      "Y=834100, Predicted=[799712.25]\n",
      "Y=332000, Predicted=[293926.97]\n",
      "Y=423900, Predicted=[606129.]\n",
      "Y=1000800, Predicted=[833976.4]\n",
      "Y=566500, Predicted=[432987.38]\n",
      "Y=920700, Predicted=[761628.9]\n",
      "Y=473200, Predicted=[614578.3]\n",
      "Y=772300, Predicted=[625373.3]\n",
      "Y=566800, Predicted=[505850.25]\n",
      "Y=653500, Predicted=[687634.7]\n",
      "Y=424000, Predicted=[710460.25]\n",
      "Y=470500, Predicted=[245657.11]\n",
      "Y=952100, Predicted=[964075.06]\n",
      "Y=571200, Predicted=[384788.8]\n",
      "Y=806100, Predicted=[740175.8]\n",
      "Y=415700, Predicted=[466869.6]\n",
      "Y=684500, Predicted=[564881.75]\n",
      "Y=631300, Predicted=[487631.66]\n",
      "Y=308200, Predicted=[525164.94]\n",
      "Y=840200, Predicted=[546393.2]\n",
      "Y=406800, Predicted=[422177.47]\n",
      "Y=59700, Predicted=[349284.12]\n",
      "Y=1050300, Predicted=[822057.9]\n",
      "Y=408900, Predicted=[461029.5]\n",
      "Y=354200, Predicted=[415255.44]\n",
      "Y=138000, Predicted=[579270.56]\n",
      "Y=762700, Predicted=[537307.44]\n",
      "Y=221900, Predicted=[366914.28]\n",
      "Y=493300, Predicted=[441778.97]\n",
      "Y=307300, Predicted=[389104.22]\n",
      "Y=494200, Predicted=[292414.88]\n",
      "Y=381900, Predicted=[432624.78]\n",
      "Y=772600, Predicted=[912004.1]\n",
      "Y=270300, Predicted=[428728.3]\n",
      "Y=467000, Predicted=[560333.25]\n",
      "Y=565600, Predicted=[647228.56]\n",
      "Y=876300, Predicted=[622676.56]\n",
      "Y=16500, Predicted=[125219.07]\n",
      "Y=80500, Predicted=[220464.64]\n",
      "Y=668200, Predicted=[301543.7]\n",
      "Y=921900, Predicted=[751124.94]\n",
      "Y=612200, Predicted=[545713.25]\n",
      "Y=892300, Predicted=[587085.1]\n",
      "Y=818600, Predicted=[737221.3]\n",
      "Y=557000, Predicted=[480753.]\n",
      "Y=524900, Predicted=[565399.8]\n",
      "Y=571000, Predicted=[460001.06]\n",
      "Y=407300, Predicted=[455271.78]\n",
      "Y=732500, Predicted=[556162.2]\n",
      "Y=386400, Predicted=[533060.2]\n",
      "Y=1023300, Predicted=[727499.25]\n",
      "Y=468800, Predicted=[513748.44]\n",
      "Y=100800, Predicted=[336413.5]\n",
      "Y=252000, Predicted=[333700.88]\n",
      "Y=1251800, Predicted=[665298.5]\n",
      "Y=317800, Predicted=[367516.78]\n",
      "Y=858900, Predicted=[745561.94]\n",
      "Y=335500, Predicted=[413432.6]\n",
      "Y=579700, Predicted=[521447.25]\n",
      "Y=54200, Predicted=[302281.12]\n",
      "Y=1042600, Predicted=[516497.03]\n",
      "Y=670300, Predicted=[528755.3]\n",
      "Y=343100, Predicted=[570560.7]\n",
      "Y=621400, Predicted=[505810.2]\n",
      "Y=549900, Predicted=[444023.03]\n",
      "Y=477200, Predicted=[471854.66]\n",
      "Y=591700, Predicted=[602297.7]\n",
      "Y=468000, Predicted=[590731.1]\n",
      "Y=355400, Predicted=[215309.36]\n",
      "Y=142200, Predicted=[330599.1]\n",
      "Y=61100, Predicted=[128553.6]\n",
      "Y=513600, Predicted=[591640.5]\n",
      "Y=828100, Predicted=[744856.94]\n",
      "Y=899400, Predicted=[754815.5]\n",
      "Y=244000, Predicted=[192359.12]\n",
      "Y=663500, Predicted=[662000.1]\n",
      "Y=560900, Predicted=[783661.56]\n",
      "Y=426000, Predicted=[227245.6]\n",
      "Y=556100, Predicted=[638417.2]\n",
      "Y=296400, Predicted=[411815.1]\n",
      "Y=554800, Predicted=[583760.44]\n",
      "Y=68100, Predicted=[298755.16]\n",
      "Y=506500, Predicted=[538661.7]\n",
      "Y=70000, Predicted=[287844.9]\n",
      "Y=860600, Predicted=[487982.66]\n",
      "Y=2000, Predicted=[569270.4]\n",
      "Y=319300, Predicted=[523326.16]\n",
      "Y=378800, Predicted=[488979.5]\n",
      "Y=132300, Predicted=[117360.59]\n",
      "Y=362300, Predicted=[266792.6]\n",
      "Y=811200, Predicted=[613735.56]\n",
      "Y=943100, Predicted=[833147.9]\n",
      "Y=374900, Predicted=[515472.34]\n",
      "Y=727100, Predicted=[744556.44]\n",
      "Y=408500, Predicted=[538323.6]\n",
      "Y=502300, Predicted=[575223.25]\n",
      "Y=925300, Predicted=[827046.56]\n",
      "Y=882800, Predicted=[632103.6]\n",
      "Y=488800, Predicted=[587085.1]\n",
      "Y=520200, Predicted=[597298.]\n",
      "Y=122300, Predicted=[498381.66]\n",
      "Y=668000, Predicted=[746432.7]\n",
      "Y=385800, Predicted=[216796.14]\n",
      "Y=211000, Predicted=[302838.97]\n",
      "Y=899500, Predicted=[926056.4]\n",
      "Y=631300, Predicted=[599131.06]\n",
      "Y=60000, Predicted=[389511.62]\n",
      "Y=270000, Predicted=[242997.44]\n",
      "Y=629700, Predicted=[705898.5]\n",
      "Y=645700, Predicted=[656381.25]\n",
      "Y=730000, Predicted=[606829.56]\n",
      "Y=338300, Predicted=[100300.664]\n",
      "Y=386200, Predicted=[554711.06]\n",
      "Y=123000, Predicted=[159051.92]\n",
      "Y=444200, Predicted=[468918.47]\n",
      "Y=600200, Predicted=[538276.8]\n",
      "Y=587600, Predicted=[386946.03]\n",
      "Y=255900, Predicted=[402710.5]\n",
      "Y=155100, Predicted=[487239.38]\n",
      "Y=102700, Predicted=[342223.06]\n",
      "Y=489000, Predicted=[685573.75]\n",
      "Y=686000, Predicted=[682283.4]\n",
      "Y=103000, Predicted=[212913.97]\n",
      "Y=502900, Predicted=[680201.7]\n",
      "Y=763300, Predicted=[538032.3]\n",
      "Y=667200, Predicted=[513168.]\n",
      "Y=645600, Predicted=[641406.1]\n",
      "Y=450400, Predicted=[463349.72]\n",
      "Y=751400, Predicted=[657293.5]\n",
      "Y=775300, Predicted=[463417.]\n",
      "Y=625900, Predicted=[589091.25]\n",
      "Y=128500, Predicted=[473153.16]\n",
      "Y=245900, Predicted=[353559.4]\n",
      "Y=202500, Predicted=[687472.3]\n",
      "Y=617700, Predicted=[552004.06]\n",
      "Y=410500, Predicted=[257765.06]\n",
      "Y=507400, Predicted=[511352.12]\n",
      "Y=441400, Predicted=[584479.3]\n",
      "Y=652300, Predicted=[553990.25]\n",
      "Y=1049600, Predicted=[833091.6]\n",
      "Y=182200, Predicted=[408640.2]\n",
      "Y=802900, Predicted=[554330.7]\n",
      "Y=612700, Predicted=[583760.44]\n",
      "Y=451400, Predicted=[593980.1]\n",
      "Y=450000, Predicted=[723488.06]\n",
      "Y=642300, Predicted=[559688.2]\n",
      "Y=833200, Predicted=[833463.4]\n",
      "Y=709800, Predicted=[530289.4]\n",
      "Y=289600, Predicted=[482699.94]\n",
      "Y=652700, Predicted=[483428.75]\n",
      "Y=752800, Predicted=[750539.7]\n",
      "Y=565500, Predicted=[460778.84]\n",
      "Y=35500, Predicted=[268331.56]\n",
      "Y=422900, Predicted=[363364.53]\n",
      "Y=559800, Predicted=[345284.97]\n",
      "Y=167100, Predicted=[304288.06]\n",
      "Y=101700, Predicted=[259188.92]\n",
      "Y=1036600, Predicted=[866973.3]\n",
      "Y=521600, Predicted=[658897.25]\n",
      "Y=133000, Predicted=[111792.31]\n",
      "Y=703800, Predicted=[821667.2]\n",
      "Y=867700, Predicted=[691215.94]\n",
      "Y=264000, Predicted=[347127.72]\n",
      "Y=201600, Predicted=[501051.4]\n",
      "Y=534900, Predicted=[466333.56]\n",
      "Y=513500, Predicted=[733789.6]\n",
      "Y=348000, Predicted=[169514.95]\n",
      "Y=567700, Predicted=[567772.75]\n",
      "Y=712000, Predicted=[535218.44]\n",
      "Y=424300, Predicted=[260753.56]\n",
      "Y=371700, Predicted=[679171.1]\n",
      "Y=595800, Predicted=[216702.42]\n",
      "Y=616200, Predicted=[405834.38]\n",
      "Y=714900, Predicted=[626189.8]\n",
      "Y=355600, Predicted=[497827.12]\n",
      "Y=676000, Predicted=[916239.7]\n",
      "Y=357700, Predicted=[232321.47]\n",
      "Y=124700, Predicted=[221925.62]\n",
      "Y=706700, Predicted=[550851.1]\n",
      "Y=604000, Predicted=[469785.1]\n",
      "Y=1073900, Predicted=[890762.6]\n",
      "Y=430700, Predicted=[337738.25]\n",
      "Y=447700, Predicted=[769035.7]\n",
      "Y=519200, Predicted=[345284.97]\n",
      "Y=459100, Predicted=[484321.9]\n",
      "Y=398600, Predicted=[508193.72]\n",
      "Y=383300, Predicted=[539501.56]\n",
      "Y=593000, Predicted=[730101.6]\n",
      "Y=616100, Predicted=[511891.84]\n",
      "Y=831200, Predicted=[842749.2]\n",
      "Y=474200, Predicted=[402928.34]\n",
      "Y=200700, Predicted=[211821.47]\n",
      "Y=508100, Predicted=[446571.06]\n",
      "Y=648000, Predicted=[552082.8]\n",
      "Y=115800, Predicted=[520160.22]\n",
      "Y=547800, Predicted=[452488.47]\n",
      "Y=404200, Predicted=[363364.53]\n",
      "Y=388700, Predicted=[525507.3]\n",
      "Y=84400, Predicted=[188810.14]\n",
      "Y=506600, Predicted=[642469.5]\n",
      "Y=751800, Predicted=[658377.2]\n",
      "Y=597500, Predicted=[450224.56]\n",
      "Y=531600, Predicted=[593256.25]\n",
      "Y=155000, Predicted=[374185.75]\n",
      "Y=1007100, Predicted=[797268.75]\n",
      "Y=1235000, Predicted=[562206.7]\n",
      "Y=380700, Predicted=[259237.06]\n",
      "Y=417700, Predicted=[521139.66]\n",
      "Y=247800, Predicted=[276645.03]\n",
      "Y=206700, Predicted=[545229.3]\n",
      "Y=458300, Predicted=[437788.7]\n",
      "Y=820800, Predicted=[679919.4]\n",
      "Y=589700, Predicted=[550964.7]\n",
      "Y=649100, Predicted=[753363.94]\n",
      "Y=370600, Predicted=[472309.6]\n",
      "Y=1085900, Predicted=[706245.44]\n",
      "Y=782600, Predicted=[550016.75]\n",
      "Y=695700, Predicted=[606129.]\n",
      "Y=369900, Predicted=[374674.22]\n",
      "Y=575500, Predicted=[404817.38]\n",
      "Y=105100, Predicted=[386944.38]\n",
      "Y=238500, Predicted=[205572.58]\n",
      "Y=690700, Predicted=[619091.94]\n",
      "Y=191500, Predicted=[132274.67]\n",
      "Y=672500, Predicted=[569648.8]\n",
      "Y=173700, Predicted=[240701.92]\n",
      "Y=440200, Predicted=[454437.8]\n",
      "Y=922700, Predicted=[727652.8]\n",
      "Y=603700, Predicted=[511557.34]\n",
      "Y=314600, Predicted=[390933.34]\n",
      "Y=468300, Predicted=[511397.03]\n",
      "Y=693300, Predicted=[841382.06]\n",
      "Y=356000, Predicted=[244227.33]\n",
      "Y=602000, Predicted=[487595.9]\n",
      "Y=1003600, Predicted=[677895.94]\n",
      "Y=525900, Predicted=[503186.4]\n",
      "Y=429400, Predicted=[501297.84]\n",
      "Y=89900, Predicted=[359817.12]\n",
      "Y=391400, Predicted=[407240.]\n",
      "Y=829700, Predicted=[618547.4]\n",
      "Y=487200, Predicted=[542844.5]\n",
      "Y=674200, Predicted=[570325.6]\n",
      "Y=255400, Predicted=[464728.8]\n",
      "Y=843200, Predicted=[770706.8]\n",
      "Y=494300, Predicted=[589610.3]\n",
      "Y=794400, Predicted=[620733.75]\n",
      "Y=266100, Predicted=[531074.75]\n",
      "Y=365900, Predicted=[515111.62]\n",
      "Y=604500, Predicted=[497137.66]\n",
      "Y=120000, Predicted=[420126.]\n",
      "Y=241600, Predicted=[472962.22]\n",
      "Y=599700, Predicted=[560719.94]\n",
      "Y=470200, Predicted=[498343.1]\n",
      "Y=736400, Predicted=[538951.1]\n",
      "Y=978800, Predicted=[328500.7]\n",
      "Y=490100, Predicted=[402928.34]\n",
      "Y=683100, Predicted=[512948.28]\n",
      "Y=442000, Predicted=[566288.75]\n",
      "Y=252000, Predicted=[429129.34]\n",
      "Y=584500, Predicted=[492969.75]\n",
      "Y=448200, Predicted=[488320.75]\n",
      "Y=376300, Predicted=[421480.34]\n",
      "Y=470500, Predicted=[561360.9]\n",
      "Y=65500, Predicted=[372217.5]\n",
      "Y=649100, Predicted=[311990.06]\n",
      "Y=500, Predicted=[250537.27]\n",
      "Y=299900, Predicted=[466611.38]\n",
      "Y=871800, Predicted=[552376.]\n",
      "Y=60000, Predicted=[176317.5]\n",
      "Y=364500, Predicted=[479839.7]\n",
      "Y=528600, Predicted=[839021.]\n",
      "Y=900000, Predicted=[654125.1]\n",
      "Y=200200, Predicted=[244387.28]\n",
      "Y=540900, Predicted=[575324.1]\n",
      "Y=359400, Predicted=[526790.8]\n",
      "Y=95500, Predicted=[202948.7]\n",
      "Y=90300, Predicted=[294882.3]\n",
      "Y=303500, Predicted=[294226.94]\n",
      "Y=195700, Predicted=[215643.6]\n",
      "Y=443000, Predicted=[556633.7]\n",
      "Y=429100, Predicted=[479143.22]\n",
      "Y=547300, Predicted=[585090.9]\n",
      "Y=717400, Predicted=[482351.1]\n",
      "Y=500, Predicted=[564564.25]\n",
      "Y=438500, Predicted=[414141.6]\n",
      "Y=934900, Predicted=[538898.5]\n",
      "Y=347300, Predicted=[329888.44]\n",
      "Y=507700, Predicted=[326803.62]\n",
      "Y=471000, Predicted=[443561.53]\n",
      "Y=473400, Predicted=[413681.6]\n",
      "Y=344600, Predicted=[557194.7]\n",
      "Y=251700, Predicted=[131621.]\n",
      "Y=665300, Predicted=[608769.56]\n",
      "Y=568200, Predicted=[491615.97]\n",
      "Y=335700, Predicted=[353988.56]\n",
      "Y=593000, Predicted=[634159.4]\n",
      "Y=756300, Predicted=[500510.25]\n",
      "Y=1087100, Predicted=[920045.06]\n",
      "Y=790700, Predicted=[899996.7]\n",
      "Y=706600, Predicted=[670065.7]\n",
      "Y=463100, Predicted=[491830.72]\n",
      "Y=42500, Predicted=[301350.22]\n",
      "Y=296900, Predicted=[383126.2]\n",
      "Y=436600, Predicted=[432675.16]\n",
      "Y=498200, Predicted=[516160.7]\n",
      "Y=517600, Predicted=[590494.5]\n",
      "Y=420000, Predicted=[374674.22]\n",
      "Y=734200, Predicted=[530174.06]\n",
      "Y=416900, Predicted=[606719.94]\n",
      "Y=583300, Predicted=[412214.66]\n",
      "Y=501800, Predicted=[656410.25]\n",
      "Y=435800, Predicted=[389546.22]\n",
      "Y=468700, Predicted=[473836.66]\n",
      "Y=108500, Predicted=[416443.53]\n",
      "Y=997500, Predicted=[818690.7]\n",
      "Y=585200, Predicted=[670224.44]\n",
      "Y=307100, Predicted=[291178.4]\n",
      "Y=779600, Predicted=[926904.1]\n",
      "Y=734000, Predicted=[647114.9]\n",
      "Y=659300, Predicted=[1008564.5]\n",
      "Y=658700, Predicted=[408512.3]\n",
      "Y=48400, Predicted=[541276.7]\n",
      "Y=592600, Predicted=[764046.75]\n",
      "Y=606600, Predicted=[647452.94]\n",
      "Y=368700, Predicted=[479958.28]\n",
      "Y=594300, Predicted=[616675.7]\n",
      "Y=755700, Predicted=[478464.47]\n",
      "Y=625300, Predicted=[543418.75]\n",
      "Y=356800, Predicted=[338037.88]\n",
      "Y=528200, Predicted=[520049.38]\n",
      "Y=1000, Predicted=[126831.5]\n",
      "Y=462400, Predicted=[608568.3]\n",
      "Y=675900, Predicted=[873409.6]\n",
      "Y=502900, Predicted=[559643.7]\n",
      "Y=585100, Predicted=[683949.75]\n",
      "Y=647600, Predicted=[647080.06]\n",
      "Y=622800, Predicted=[463130.1]\n",
      "Y=371000, Predicted=[355581.53]\n",
      "Y=1159700, Predicted=[639535.56]\n",
      "Y=336000, Predicted=[558382.5]\n",
      "Y=475600, Predicted=[676593.7]\n",
      "Y=448300, Predicted=[635391.06]\n",
      "Y=650400, Predicted=[526544.56]\n",
      "Y=357300, Predicted=[412599.72]\n",
      "Y=287400, Predicted=[261170.55]\n",
      "Y=239900, Predicted=[556323.44]\n",
      "Y=465400, Predicted=[689075.25]\n",
      "Y=374700, Predicted=[444023.03]\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted vs actual results\n",
    "for i in range(len(y_test)):\n",
    "    print(\"Y=%s, Predicted=%s\" % (y_test[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139965.8096638363\n",
      "32372502253.333073\n",
      "179923.60115708297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3621736639658593"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "(np.sqrt(metrics.mean_squared_error(y_test, y_pred))/\n",
    "np.mean(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
